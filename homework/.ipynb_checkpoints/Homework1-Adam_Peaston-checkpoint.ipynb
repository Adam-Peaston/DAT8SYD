{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYD DAT 8 Lab 1 - Git and Markdown\n",
    "\n",
    "## Homework:\n",
    "\n",
    "#### Setup\n",
    "* Resolve any installation issues before next class.\n",
    "* Make sure you have a github profile and created a repo called \"SYD_DAT_8\"\n",
    "* Clone the class repo (this one!)\n",
    "* Review this [code](../labs/Week 1/00_python_refresher.py) for a recap of some Python basics.\n",
    "\n",
    "#### Communication\n",
    "* Read [Analyzing the Analyzers](http://cdn.oreillystatic.com/oreilly/radarreport/0636920029014/Analyzing_the_Analyzers.pdf) for a useful look at the different types of data scientists. Write down 5 key points you took away from the article\n",
    "1. \n",
    "> Data Science is a broad term that encapsulates and wide variety of skills and techniques\n",
    "2. \n",
    "> Many companies wanting to build data science capability have no idea what skills they really need\n",
    "3. \n",
    "> Many companies wanting to recruit data scientists are poor at clearly articulating the requirements of the roles\n",
    "4. \n",
    "> Data Creatives and Developers tend to have more varied skillsets than Business people or Researchers.\n",
    "5. \n",
    "> Most data scientist ID groups spent most of their time working on data sets at the scale of gigabytes or less.\n",
    "\n",
    "* Read about some [Markdown Techniques](http://daringfireball.net/projects/markdown/syntax)\n",
    "* Write a summary of 2 chapters of [The Data Science Handbook](http://www.thedatasciencehandbook.com/) in Markdown in this notebook and commit it to your repository on github\n",
    "\n",
    "1. \n",
    "> JACE KOHLMEIER at Khan Academy - Jace finished a degree in Math and Computer Science and then entered a PhD program but found more reward in the start-up commercial sector immediately. He ditched the PhD, wrapped up a Masters instead and went to work at a hedge fund where he developed skills building empirical models. He later turned his attention to his 'pet vision' of using high frequency feedback loops to test educational content and pedagogy whereupon he discovered Sal Khan of Khan Academy via TED talks. Jace believes the hardest thing to learn on the job is the quantitative techniques underlying statistical learning - the math - but also stresses fluency of coding as an essential skill for successful data scientists.\n",
    "\n",
    "2. \n",
    "> DREW CONWAY at Project Florida - Drew was the original creator of the iconic Data Science Venn Diagram. He says that since it's inception he's realised that it is missing a key element of data science which is the skill in conveying the kind of complex, often technical information generated for and by the techniques used in data science to non-technical audiences. Without this skill - which is strongly associated with the data visualisation element of this course - the technical aspects are of little value because they will not be actualised without the confidence of non-technical recipients of that information. Drew points out “Human problems won’t be solved by root mean square error.” Human problems are much more complex, nuanced and subtle than can ordinarily be expressed in even the most complex statistical models which is a perspective and a context that data scientists would do well to recognise and hold front of mind when developing models and communicating their results.\n",
    "\n",
    "#### Programming\n",
    "* Complete the lab from class and the additional Exercise below\n",
    "\n",
    "#### Course Project\n",
    "* Come up with 5 different ideas for your course project. For each one list:\n",
    "  * Overview of your idea\n",
    "  * What data you will use\n",
    "  * What the outcome is that you are trying to achieve\n",
    "  * Any ideas of modelling techniques it may involve\n",
    "  \n",
    "1.\n",
    ">1. Zillow Price Kaggle Competition [Overview] Develop a predictive model to predict the log-error of 'Zestimates' - house prices in the US - based on features within the data set provided. [What data] Data provided through Kaggle - historical property features and corresponding log-errors. [Outcome] As accurate a predictive model as possible - minimum log-error. [Modelling techniques] - Linear/non-linear regression modelling, nearest neighbors modelling all potentially fruitful techniques. May look at feature engineering or supplementing with external data sets. It would be interesting to mesh a nearest-neighbours analysis based on geographical location with typical linear/non-linear regression model based on the nearest neighbours 'neighborhood coefficient' and features within the data sets such as number of bedrooms etc. Possibly even supplement with other datasets such as local weather conditions and day of the week on the sale date.\n",
    "\n",
    "2.\n",
    ">2. MovieLens 20M Dataset Kaggle Data set [Overview] Movies are rated and tagged by users. Each user rates at least 20 movies. The data set contains 138493 users and 20000263 ratings of 27278 movies from 1995 to essentially the present. [What data] Data provided through kaggle - all datasets necessary available. [Outcome] Uncertain - maybe a recommendation engine, maybe a rating prediction, maybe an exploration of temporal trends in genre popularity. [Motelling techniques] Matrix factorisation and dimensionality reduction, regression, nearest-neighbours clustering.\n",
    "\n",
    "3.\n",
    ">3. Imminent watermain failure [Overview] Sydney Water measures and records flow through certain points in their network. These are used to carry out mass-balance calculations to test performance and audit supply, as well as calculate regional demand rates. [What data] Data would be downloaded from Sydney Water in the form of a large number of flow measurement files. Network geometry information would also need to be supplemented to understand the measurement location's relationships to each other. Labelled examples of flow patterns attending main breaks would be more difficult to find. [Outcome] A model that can accurately identify imminent main failures based on mass-balance calculations or on flow characteristics [Modelling techniques] Regression and potentially neural networks. If a convnet could be trained to recognise the difference between 'typical' and 'atypical' flow patters, that would be cool.\n",
    "\n",
    "4.\n",
    ">4. Raw water turbidity [Overview] Drinking water for Sydney is supplied predominantly by the Prospect Water Filtration Plant which recieves raw water from Warragamba Dam. Raw water arriving at the Prospect plant is of variable quality, particularly with regard to measured 'turbidity' and 'colour'. These variables can affect the performance of the plant and impact shut-down times for maintenance. [Data] Daily raw water turbidity and colour measurements taken at the inlet to the Prospect WFP as well as historical level measurements of Warragamba Dam and regional weather data. [Outcome] A model that can accurately predict raw water turbidity and colour and thereby improve the Sydney Water's capacity to plan and react. [Modelling techniques] Again, regression would probably be a first place to start. Deeper neural networks might also have application as the natural extension to the simple regression model.\n",
    "\n",
    "5.\n",
    ">5. I could only think of 4.\n",
    "\n",
    "**Instructions: copy this file and append your name in the filename, e.g. Homework1_ian_hansel.ipynb.\n",
    "Then commit this in your local repository, push it to your github account so I can see your work. Remeber if you get stuck to look at the slides going over Git**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Four - Movie Lens Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28Mstudent55414     2\n",
      "17Mstudent60402     2\n",
      "44Fother60630       2\n",
      "51Meducator20003    2\n",
      "32Mstudent97301     2\n",
      "21Fstudent55414     2\n",
      "27Fother20009       2\n",
      "Name: unique, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# for each occupation in 'users', count the number of occurrences\n",
    "\n",
    "import pandas as pd\n",
    "user_cols = ['user_id', 'age', 'gender', 'occupation', 'zip_code']\n",
    "users = pd.read_table('../data/u.user', sep='|', header=None, names=user_cols, index_col='user_id', dtype={'zip_code':str})\n",
    "users.head()\n",
    "\n",
    "# for each occupation, calculate the mean age\n",
    "users_occ_agesum = users[['occupation', 'age']].groupby('occupation').sum()\n",
    "users_occ_count = users[['occupation', 'age']].groupby('occupation').count()\n",
    "users_occ_avgage = users_occ_agesum / users_occ_count\n",
    "\n",
    "#or alternatively \n",
    "users_occ_meanage = users[['occupation', 'age']].groupby('occupation').mean()\n",
    "\n",
    "#print(users_occ_avgage)\n",
    "#print(users_occ_meanage)\n",
    "\n",
    "# for each occupation, calculate the minimum and maximum ages\n",
    "users_occ_minage = users[['occupation', 'age']].groupby('occupation').min()\n",
    "users_occ_maxage = users[['occupation', 'age']].groupby('occupation').max()\n",
    "#print(users_occ_minage)\n",
    "#print(users_occ_maxage)\n",
    "\n",
    "# for each combination of occupation and gender, calculate the mean age\n",
    "users_m = users[users.gender=='M']\n",
    "users_f = users[users.gender=='F']\n",
    "\n",
    "users_m_occ_meanage = users_m[['occupation', 'age']].groupby('occupation').mean()\n",
    "users_f_occ_meanage = users_f[['occupation', 'age']].groupby('occupation').mean()\n",
    "\n",
    "#print(users_f_occ_meanage)\n",
    "#print(users_m_occ_meanage)\n",
    "\n",
    "# randomly sample a DataFrame\n",
    "from sklearn.cross_validation import train_test_split\n",
    "random_sample = train_test_split(users, random_state=1)\n",
    "#print(random_sample)\n",
    "\n",
    "# detect duplicate users\n",
    "users[\"unique\"] = users[\"age\"].map(str) + users[\"gender\"] + users[\"occupation\"] + users['zip_code'].map(str)\n",
    "user_counts = users[\"unique\"].value_counts()\n",
    "duplicates = user_counts[user_counts > 1]\n",
    "print(duplicates)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
