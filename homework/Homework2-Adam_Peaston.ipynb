{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYD DAT 8 Homework 2 - Visualisation and Regression\n",
    "\n",
    "## Homework - Due Friday 30th June\n",
    "\n",
    "#### Setup\n",
    "* Signup for an AWS account\n",
    "\n",
    "#### Communication\n",
    "* Imagine you are trying to explain to someone what Linear Regression is - but they have no programming/maths experience? How would you explain the overall process, what a p-value means and what R-Squared means?\n",
    "\n",
    "Linear regression is the technique of finding a relatively simple equation that tells us what we should expect an unknown thing to be given a handful of known things. When the the equation is as good as we can get it at predicting the unknown thing, we measure how accurate it is. The 'p' value measures how likely it is that the degree to which our model does better than random chance could be due to random chance. The 'r-squared' value measures how close our predictions are to real observations.\n",
    "\n",
    "* Read the paper [Useful things to know about machine learning]( https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf). \n",
    "    * What have we covered so far from this paper? \n",
    "    \n",
    "    Bias/variance, evaluation/optimisation, overfitting, logistic regression, KNN, decisions trees, the curse of dimensionalty. We haven't covered naive bayes, support vector machines, bayesian model averaging, and I get a little confused with the notion of a representation and a 'learner'.\n",
    "    \n",
    "    * Explain sections 6-13 in your own words\n",
    "    \n",
    "    Section 6 - intuitions in 3 or less dimensions break down in higher dimensions. Higher dimensions effectively introduce exponentially more flexibility to any model we build and so rapidly increase the problem of over-fitting.\n",
    "    \n",
    "    Section 7 - Even though machine learning is based on inductive reasoning, there are rigorous mathematical principles that justify confidence in the results obtained.\n",
    "    \n",
    "    Section 8 - Feature engineering is a powerful and necessary process for learning complex structures in the data.\n",
    "    \n",
    "    Section 9 - Simpler models can perform as well as more complicated ones and require less data to train, while being more interpretable for the users.\n",
    "    \n",
    "    Section 10 - Being profficient in many types of models and constructing ensembles of models is much better than having a favourite and trying to use it for everything.\n",
    "    \n",
    "    Section 11 - Simpler models are better for reasons other than accuracy.\n",
    "    \n",
    "    Section 12 - A model might be flexible enough to encode any function, simple or arbitrarily complex, but that model might be harder to train suitably because of it's flexibility or complexity.\n",
    "    \n",
    "    Section 13 - Causation / correlation - who cares. The point is we can with confidence say 'if A, then B'.\n",
    "\n",
    "#### Machine Learning\n",
    "* Describe 3 ways we can select what features to use in a model\n",
    "\n",
    "Forward and backward stepwise selection (there's a hybrid of these the name of which escapes me), ridge regression, the lasso and elastic net.\n",
    "\n",
    "* Complete the first 3 exercises from Chapter 3 of Introduction to Statistical Learning in Python\n",
    "\n",
    "'1.' Describe the Null Hypothesis - the Null Hypothesis for each of these variables is that the significance of this variable is due to variance in the data (noise) rather than true correlation. The 'Intercept', TV and radio p-values of less than 0.0001 are strong evidence that these variables really do impact sales revenue. The 'intercept' variable meaning that sales revenue is going to occur even without any advertising. The high p-value for newspaper of 0.8599 indicates that it is likely the estimated impact of newspaper advertising on sales revenue is due to random variation in the data and not real significance.\n",
    "\n",
    "'2.' KNN classifiers assign categorical classification based on the categorical classification of other instances which are 'nearby' to the instance in question by some measure of distance in the features. KNN regression assigns a numerical value to the instance in question as typically some distance-weighted average of numerical values of 'nearby' instances.\n",
    "\n",
    "'3'. (a) Statement iii. is true because the negatively weighted interaction term between GPA and Gender overwhelms the positively weighted gender variable for high enough GPA.\n",
    "(b) $137.1\n",
    "(c) False. A measure of significance such as a p value would be required to make such a claim.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Course Project\n",
    "* For the following setup a new github repository for your project and share it with Alasdair and Ian over Slack.\n",
    "* Load the data you have gathered for your project into Python and run some summary statistics over the data. Are there any interesting features of the data that jump out? (Include the code)\n",
    "* Draft/Sketch (or wireframe) some data visualisations that would be useful for you to explore your data set\n",
    "* Are there any regresion or clustering techniques you could use in your project? Write them down (with the corresponding scikit learn function) and what you think you would get out of it. Try it out if you get a chance.\n",
    "\n",
    "I think the elsaticnet regression algorithm would be of use with this dataset as there are a large number of variables, making feature selection an important step.\n",
    "\n",
    "\n",
    "**Instructions: copy this file and append your name in the filename, e.g. Homework2_ian_hansel.ipynb.\n",
    "Then commit this in your local repository, push it to your github account and create a pull request so I can see your work. Remeber if you get stuck to look at the slides going over Fork, Clone, Commit, Push and Pull request.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
